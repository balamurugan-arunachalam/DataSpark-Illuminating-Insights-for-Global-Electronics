{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSpark : Illuminating Insights for Global Electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "customers = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Customers.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Handling Missing Values\n",
    "customers['City'].fillna('Unknown', inplace=True)\n",
    "customers['State'].fillna('Unknown', inplace=True)\n",
    "customers['State Code'].fillna('Unknown', inplace=True)  \n",
    "customers['Zip Code'].fillna(0, inplace=True)\n",
    "customers['Country'].fillna('Unknown', inplace=True)\n",
    "customers['Continent'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Correcting Data Types\n",
    "customers['Birthday'] = pd.to_datetime(customers['Birthday'], errors='coerce')\n",
    "\n",
    "# Removing Duplicates (excluding primary key and unique key)\n",
    "customers.drop_duplicates(subset=[col for col in customers.columns if col not in ['CustomerKey']], inplace=True)\n",
    "\n",
    "# Renaming Columns for Consistency\n",
    "customers.rename(columns={'CustomerKey': 'Customer_ID', 'Zip Code': 'Zip_Code', 'State Code': 'State_Code'}, inplace=True)\n",
    "\n",
    "# Handling Categorical Data\n",
    "customers['Gender'] = customers['Gender'].astype('category')\n",
    "\n",
    "# Calculate Age\n",
    "customers['Age'] = (pd.Timestamp.now() - customers['Birthday']).dt.total_seconds() / (60*60*24*365.25)\n",
    "customers['Age'] = customers['Age'].astype(int)\n",
    "\n",
    "# Save cleaned dataset\n",
    "customers.to_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Customers.csv', index=False)\n",
    "\n",
    "print(\"Customers data cleaning completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "sales = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Sales.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Define primary and unique key columns\n",
    "primary_keys_sales = ['Order Number', 'Line Item', 'CustomerKey', 'StoreKey', 'ProductKey']\n",
    "\n",
    "# Handling Missing Values\n",
    "sales.dropna(subset=['Order Date'], inplace=True)\n",
    "\n",
    "# Correcting Data Types\n",
    "sales['Order Date'] = pd.to_datetime(sales['Order Date'], errors='coerce')\n",
    "\n",
    "# Handling Outliers (e.g., capping extreme values in Quantity)\n",
    "sales['Quantity'] = np.where(sales['Quantity'] > sales['Quantity'].quantile(0.99), sales['Quantity'].quantile(0.99), sales['Quantity'])\n",
    "\n",
    "# Removing Duplicates\n",
    "sales.drop_duplicates(inplace=True)\n",
    "\n",
    "# High Percentage of Missing Values\n",
    "threshold = 0.5\n",
    "columns_to_drop = [col for col in sales.columns if sales[col].isnull().sum() / len(sales) > threshold and col not in primary_keys_sales]\n",
    "\n",
    "# Drop irrelevant, low variance, redundant, high correlation, and privacy-sensitive columns as per conditions\n",
    "sales.drop(columns=set(columns_to_drop), inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "sales.to_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Sales.csv', index=False)\n",
    "\n",
    "print(\"Sales data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0   1/1/2008\n",
      "1         2  Australia            Northern Territory          665.0  1/12/2008\n",
      "2         3  Australia               South Australia         2000.0   1/7/2012\n",
      "3         4  Australia                      Tasmania         2000.0   1/1/2010\n",
      "4         5  Australia                      Victoria         2000.0  12/9/2015\n",
      "Stores data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "stores = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Stores.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Display the first few rows to understand the data\n",
    "print(stores.head())\n",
    "\n",
    "# Handling Missing Values\n",
    "\n",
    "stores['Country'].fillna('Unknown', inplace=True)\n",
    "stores['State'].fillna('Unknown', inplace=True)\n",
    "stores['Square Meters'].fillna(0, inplace=True)\n",
    "\n",
    "# Correcting Data Types\n",
    "stores['Open Date'] = pd.to_datetime(stores['Open Date'], errors='coerce')\n",
    "\n",
    "# Removing Duplicates (excluding primary key)\n",
    "stores.drop_duplicates(subset=[col for col in stores.columns if col not in ['StoreKey']], inplace=True)\n",
    "\n",
    "# Handle Inconsistent Data (if any)\n",
    "\n",
    "# Save cleaned dataset\n",
    "stores.to_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Stores.csv', index=False)\n",
    "\n",
    "print(\"Stores data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Unit Cost USD to numeric successfully.\n",
      "Converted Unit Price USD to numeric successfully.\n",
      "Products data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "products = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Products.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Handling Missing Values\n",
    "products['Color'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Removing Duplicates\n",
    "products.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove currency symbols before converting to numeric\n",
    "for column in ['Unit Cost USD', 'Unit Price USD']:\n",
    "    products[column] = products[column].str.replace('$', '').str.replace(',', '')\n",
    "\n",
    "# Correcting Data Types (e.g., converting cost and price to numeric)\n",
    "for column in ['Unit Cost USD', 'Unit Price USD']:\n",
    "    try:\n",
    "        products[column] = pd.to_numeric(products[column], errors='coerce')\n",
    "        print(f\"Converted {column} to numeric successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {column} to numeric: {e}\")\n",
    "\n",
    "# Additional step to handle missing values in cost and price columns\n",
    "for column in ['Unit Cost USD', 'Unit Price USD']:\n",
    "    if products[column].isnull().any():\n",
    "        products[column].fillna(products[column].median(), inplace=True)\n",
    "        print(f\"Filled missing values in {column} with the median.\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "products.to_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Products.csv', index=False)\n",
    "\n",
    "print(\"Products data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exchange rates data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "exchange_rates = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Exchange_Rates.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Correcting Data Types\n",
    "exchange_rates['Date'] = pd.to_datetime(exchange_rates['Date'], errors='coerce')\n",
    "\n",
    "# Removing Duplicates\n",
    "exchange_rates.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "exchange_rates.to_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Exchange_Rates.csv', index=False)\n",
    "\n",
    "print(\"Exchange rates data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merging completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned datasets\n",
    "customers = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Customers.csv', encoding='ISO-8859-1')\n",
    "sales = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Sales.csv', encoding='ISO-8859-1')\n",
    "stores = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Stores.csv', encoding='ISO-8859-1')\n",
    "products = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Products.csv', encoding='ISO-8859-1')\n",
    "exchange_rates = pd.read_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Cleaned_Exchange_Rates.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Merge sales with customers\n",
    "sales_customers = pd.merge(sales, customers, how='left', left_on='CustomerKey', right_on='Customer_ID')\n",
    "\n",
    "# Merge sales_customers with stores\n",
    "sales_customers_stores = pd.merge(sales_customers, stores, how='left', left_on='StoreKey', right_on='StoreKey')\n",
    "\n",
    "# Merge sales_customers_stores with products\n",
    "sales_customers_stores_products = pd.merge(sales_customers_stores, products, how='left', left_on='ProductKey', right_on='ProductKey')\n",
    "\n",
    "# Optionally, merge with exchange rates if needed (e.g., if there are columns that need exchange rates)\n",
    "# Assuming you might want to join on a common date column, which isn't directly mentioned in the given datasets\n",
    "# sales_customers_stores_products = pd.merge(sales_customers_stores_products, exchange_rates, how='left', left_on='Date', right_on='Date')\n",
    "\n",
    "# Save the merged dataset\n",
    "sales_customers_stores_products.to_csv('D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/Merged_data.csv', index=False)\n",
    "\n",
    "print(\"Data merging completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame columns: 30\n",
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV data\n",
    "file_path = 'D:/Bala DS/Data science Class materials/Projects/P2_DataSpark/Project Final/Dataset/merged_data.csv'\n",
    "merged_data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace NaN values with None to handle nulls in MySQL\n",
    "merged_data = merged_data.replace({np.nan: None})\n",
    "\n",
    "# Drop the extra columns that are not part of the insert query\n",
    "merged_data = merged_data.drop(['Name', 'State_Code'], axis=1)\n",
    "\n",
    "# Check if the number of columns matches the expected number (30)\n",
    "print(f\"DataFrame columns: {len(merged_data.columns)}\")  # Should print 30\n",
    "\n",
    "# Establish MySQL connection\n",
    "conn = pymysql.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"root\",\n",
    "    password=\"Appaamma@123\",\n",
    "    database=\"final_analysis\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table if it does not exist\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS merged_data (\n",
    "        Order_Number INT,\n",
    "        Line_Item INT,\n",
    "        Order_Date DATE,\n",
    "        CustomerKey INT,\n",
    "        StoreKey INT,\n",
    "        ProductKey INT,\n",
    "        Quantity FLOAT,\n",
    "        Currency_Code VARCHAR(10),\n",
    "        Customer_ID INT,\n",
    "        Gender VARCHAR(10),\n",
    "        City VARCHAR(100),\n",
    "        State_x VARCHAR(100),\n",
    "        Zip_Code VARCHAR(20),\n",
    "        Country_x VARCHAR(100),\n",
    "        Continent VARCHAR(50),\n",
    "        Birthday DATE,\n",
    "        Age INT,\n",
    "        Country_y VARCHAR(100),\n",
    "        State_y VARCHAR(100),\n",
    "        Square_Meters FLOAT,\n",
    "        Open_Date DATE,\n",
    "        Product_Name VARCHAR(255),\n",
    "        Brand VARCHAR(100),\n",
    "        Color VARCHAR(50),\n",
    "        Unit_Cost_USD FLOAT,\n",
    "        Unit_Price_USD FLOAT,\n",
    "        SubcategoryKey INT,\n",
    "        Subcategory VARCHAR(100),\n",
    "        CategoryKey INT,\n",
    "        Category VARCHAR(100)\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Insert query template\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO merged_data (\n",
    "    Order_Number, Line_Item, Order_Date, CustomerKey, StoreKey, ProductKey, \n",
    "    Quantity, Currency_Code, Customer_ID, Gender, City, State_x, \n",
    "    Zip_Code, Country_x, Continent, Birthday, Age, Country_y, State_y, Square_Meters, \n",
    "    Open_Date, Product_Name, Brand, Color, Unit_Cost_USD, Unit_Price_USD, SubcategoryKey, \n",
    "    Subcategory, CategoryKey, Category\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Loop through each row and insert into the database\n",
    "for index, row in merged_data.iterrows():\n",
    "    # Convert the row to a tuple\n",
    "    row_tuple = tuple(row)\n",
    "    \n",
    "    # Execute the insert query\n",
    "    cursor.execute(insert_query, row_tuple)\n",
    "\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data inserted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
